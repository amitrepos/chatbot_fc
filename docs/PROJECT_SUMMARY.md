# FlexCube AI Assistant (Ask-NUO) - Project Summary

**Last Updated:** 2025-12-14
**Status:** Paused - Ready to resume

---

## ğŸ¯ What We Built

A **RAG-based AI assistant** for Oracle FlexCube banking software that:
- Answers questions from uploaded FlexCube documentation
- Analyzes screenshots of FlexCube errors using LLaVA vision model
- Falls back to LLM general knowledge for non-FlexCube questions
- Runs 100% locally (no cloud APIs) for banking data privacy

---

## âœ… Completed Features (Phases 1-5)

### Phase 1: Infrastructure âœ…
- Rocky Linux server (16 vCPU, 32GB RAM)
- Docker + Docker Compose
- Ollama with Mistral 7B (text) + LLaVA 7B (vision)
- Qdrant vector database

### Phase 2: RAG Pipeline âœ…
- LlamaIndex framework
- BGE-large embeddings (1024 dimensions)
- Document loaders (PDF, DOCX, TXT)
- Text chunking (500 tokens, 50 overlap)
- **Two-tier query flow:**
  1. Search RAG knowledge base first
  2. Fall back to LLM general knowledge if RAG irrelevant

### Phase 3: API Layer âœ…
| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Health check |
| `/api/query` | POST | Text queries |
| `/api/query/image` | POST | Screenshot queries |
| `/api/documents` | GET | List documents |
| `/api/documents/upload` | POST | Upload & auto-index |
| `/api/documents/{filename}` | DELETE | Delete document |
| `/api/documents/reindex` | POST | Rebuild index |

### Phase 4: User Interface âœ…
- Modern tabbed interface (Text / Image / Documents)
- Conversation history (localStorage)
- Document management with drag & drop
- Auto-indexing on upload
- Smart source attribution

### Phase 5: Vision Support âœ…
- LLaVA integration for screenshot analysis
- Extracts: error codes, messages, screen names
- Connects extracted info to RAG pipeline

---

## ğŸš§ Not Yet Done (Phase 6)

### Production Hardening
- [ ] Nginx reverse proxy
- [ ] SSL/HTTPS (Let's Encrypt)
- [ ] Rate limiting
- [ ] Security (deploy user, fail2ban)
- [ ] Systemd service (auto-restart)
- [ ] Monitoring & alerts

---

## ğŸ—‚ï¸ Key Files

```
/var/www/chatbot_FC/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/main.py           # FastAPI + Web UI (1397 lines)
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ query_engine.py   # Two-tier query logic
â”‚   â”‚   â”œâ”€â”€ pipeline.py       # RAG orchestration
â”‚   â”‚   â”œâ”€â”€ vision.py         # LLaVA integration
â”‚   â”‚   â”œâ”€â”€ ollama_llm.py     # Mistral wrapper
â”‚   â”‚   â”œâ”€â”€ embeddings.py     # BGE embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py   # Qdrant integration
â”‚   â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”‚   â””â”€â”€ chunking.py
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_query_logic.py  # 19 unit tests
â”œâ”€â”€ data/documents/           # Uploaded FlexCube docs
â”œâ”€â”€ docker/docker-compose.yml # Qdrant deployment
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ cleanup.sh            # Full removal script
â”‚   â”œâ”€â”€ docker-build-push.sh  # Docker Hub push
â”‚   â””â”€â”€ deploy-new-server.sh  # Fresh deployment
â”œâ”€â”€ Dockerfile                # App container
â”œâ”€â”€ docker-compose.full.yml   # Complete stack
â”œâ”€â”€ requirements.txt
â””â”€â”€ start_api.sh
```

---

## ğŸ”§ Important Technical Details

### Two-Tier Query Flow (query_engine.py)
1. Check if question contains FlexCube keywords
2. Query RAG with retrieved context
3. If LLM says context is irrelevant AND question is general:
   - Call LLM again WITHOUT RAG context
   - Return answer from general knowledge
   - Sources = [] (empty)
4. If FlexCube question: always show document sources

### Unit Tests
```bash
cd /var/www/chatbot_FC
source venv/bin/activate
python -m pytest src/tests/test_query_logic.py -v
```
**Always run after changes to query_engine.py!**

### Start Server
```bash
cd /var/www/chatbot_FC
source venv/bin/activate
nohup python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 > api.log 2>&1 &
```

### Access
- Web UI: http://65.109.226.36:8000
- API Docs: http://65.109.226.36:8000/docs

---

## ğŸ“‹ Known Issues / Future Improvements

1. **Response time:** 20-120 seconds (CPU inference is slow)
2. **No auto-restart:** Server stops if system reboots
3. **No HTTPS:** Currently HTTP only
4. **Docker image:** Built but not pushed (need private repo)

---

## ğŸ”‘ Credentials & Access

- **Server IP:** 65.109.226.36
- **OS:** Rocky Linux
- **Docker Hub username:** amitrepos291 (image built, not pushed)
- **App Port:** 8000
- **Qdrant Port:** 6333
- **Ollama Port:** 11434

---

## ğŸ“ To Resume Development

1. SSH into server
2. Check if app is running: `curl http://localhost:8000/health`
3. If not running, start with: `bash /var/www/chatbot_FC/start_api.sh`
4. Check logs: `tail -f /var/www/chatbot_FC/api.log`
5. Run tests: `python -m pytest src/tests/test_query_logic.py -v`

---

## ğŸ“š Documentation Files

- `docs/PROJECT_SPEC.md` - Original requirements
- `docs/IMPLEMENTATION_PLAN.md` - Phase breakdown
- `docs/STATUS.md` - Current completion status
- `docs/ARCHITECTURE.md` - System architecture
- `docs/TECH_STACK.md` - Technology choices
- `Updates.md` - Change log with timestamps


